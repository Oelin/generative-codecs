{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Yxs-FXKH_-ZR",
        "3peyigj8T5wu"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generative Codecs"
      ],
      "metadata": {
        "id": "maWLKIhu-ZUB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we explore how to use codecs as generative models by sampling from their implicit distributions."
      ],
      "metadata": {
        "id": "gCq9-Z-0_G5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introduction"
      ],
      "metadata": {
        "id": "Yxs-FXKH_-ZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A codec is an algorithm for compressing and decompressing data, often of a specific modality such as text or video. Codecs consist of a encoder/code, which describes data in a more concise form, and a decoder which reconstructs data from its encoding. We'll be concerning ourselves mainly with the former."
      ],
      "metadata": {
        "id": "-gtA4gO6AB9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $L_C(x)$ denote the length of $x$ when compressed using some code $C(\\cdot)$."
      ],
      "metadata": {
        "id": "TXbsu2-8BRW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the Kraft-McMillan inequality, if $C$ is uniquely decodable, then there exists a probability distribution $p_C$, such that $p_C(x) = 2^{- L_C(x)}$. In other words, for any uniquely decodable code, we can always find a *statistical model* which produces matching code-lengths.  "
      ],
      "metadata": {
        "id": "wjPZVq6ULA7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, since $p_C$ is a generative model, it should in theory be possible to sample from it. In this way, codecs can be used to *create* data rather than just compress it. Now, for many codecs, the generated data is unlikely to be very interesting because they aren't tuned to *specific* sources such as \"English text.\" Nonetheless, this exercise is useful for elucidating the implicit statistical assumptions present in human-designed compression schemes. "
      ],
      "metadata": {
        "id": "VCDIMM_yST39"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Method"
      ],
      "metadata": {
        "id": "3peyigj8T5wu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's be a little more precise about what we're attempting to do. "
      ],
      "metadata": {
        "id": "otMUmjZhUKaw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given some code $C(\\cdot)$, we wish to sample from $p_C$ where $p_C(x) = 2^{-L_C(x)}$. "
      ],
      "metadata": {
        "id": "lZephgLXUQ3O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. At each time step $t$, we compute the distribution $p_C(x_1,\\dots,x_t) = 2^{-{L_C(x_1,\\dots,x_t)}}$. We then use top-k sampling to redistribute the probability mass amoungst $k$ *most likely* sequences."
      ],
      "metadata": {
        "id": "SsVjjn-IVS-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. We then randomly sample from the top-k distribution and return to step 1."
      ],
      "metadata": {
        "id": "rLFdvHmKZFXm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Implementation"
      ],
      "metadata": {
        "id": "ZLtoj_mHYNkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zlib\n",
        "import math\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "tKX1TRewVR_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Codec sampler class...\n",
        "\n",
        "class CodecSampler:\n",
        "\n",
        "    def __init__(self, code, alphabet=256):\n",
        "        self.code = code\n",
        "        self.alphabet = alphabet\n",
        "    \n",
        "\n",
        "    def length(self, sequence):\n",
        "        return len(self.code(sequence)) * 8\n",
        "    \n",
        "\n",
        "    def complete(self, prompt, length=10, width=2, height=3):\n",
        "        \n",
        "        queue = [ prompt ]\n",
        "        result = []\n",
        "\n",
        "\n",
        "        while len(queue):\n",
        "\n",
        "            sequence = queue.pop()\n",
        "            lengths = [self.length(sequence + [token]) for token in range(self.alphabet)]\n",
        "\n",
        "            # Find shortest encodings...\n",
        "\n",
        "            shortest = torch.topk(torch.tensor(lengths), k=width, largest=False)\n",
        "\n",
        "            for token in shortest.indices:\n",
        "\n",
        "                child = sequence + [token.item()]\n",
        "                array = result if len(child) == length else queue\n",
        "                array.append(child)\n",
        "        \n",
        "        lengths = [self.length(res) for res in result]\n",
        "        shortest = torch.topk(torch.tensor(lengths), k=height, largest=False)\n",
        "\n",
        "        return torch.tensor(result)[shortest.indices]\n",
        "\n",
        "    # def complete(self, sequence, size=10, width=2, height=3):\n",
        "\n",
        "    #     sequence = list(sequence)\n",
        "\n",
        "    #     for i in range(size):\n",
        "\n",
        "    #         lengths = [self.length(sequence + [token]) for token in range(self.alphabet)]\n",
        "    #         shortest = torch.topk(torch.tensor(lengths), k=width, largest=False)\n",
        "\n",
        "    #         probabilities = [1 / (2 ** (8 * (length/len(sequence)))) for length in shortest.values]\n",
        "    #         probabilities = np.array(probabilities)\n",
        "    #         probabilities = probabilities / probabilities.sum()\n",
        "\n",
        "\n",
        "    #         # TODO: apply tempurature\n",
        "\n",
        "    #         choice = np.random.choice(shortest.indices, p=probabilities)\n",
        "    #         sequence += [choice]\n",
        "        \n",
        "    #     return sequence\n",
        "        \n",
        "\n"
      ],
      "metadata": {
        "id": "2oQhe56seZIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip"
      ],
      "metadata": {
        "id": "r4vcxULjyPgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = CodecSampler(lambda x: zlib.compress(bytes(x)))\n",
        "b = CodecSampler(lambda x: gzip.compress(bytes(x)))"
      ],
      "metadata": {
        "id": "hCo9CTJuYquh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''.join(vocabulary[x] for x in (a.complete(prompt_tokenized, size=20)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "ippjC-kagFAD",
        "outputId": "8dabca41-907e-4766-bda0-1bea8ff12998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' nucleus sampling also called top p sampling is a more advanced version of top sampling that results in a more consistent sampling performance it cuts between selectable and non selectable tokens based on the sum of their probabilities totaled bar heights when going from left to right on the linked picture until the specified cut value p is reached as opposed to top k sampling which cuts based on position index it can be used similarity to top k sampling you can combine low value of nucleus sampling with high value of randomness while the other sampling methods set to off to break out of loops with coherence once you ve got your head around how these work you can try layering settings it is difficult to explain sampling methods in simple terms without getting sloppy but it is not a complicated concept try to visualize the process using the bars as crutches randomness controls bar heights while sampling controls where the dividing line will be between bars that can be selected and bars tokens that will be discarded from participating in theetkeetetkeetetkeetkeetetetkeetetetetetet'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 625
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wow! This is really cool. `zlib` successfully continues the alternting pattern. Let's see what other patterns it tends to recognise."
      ],
      "metadata": {
        "id": "PEMT9HGPr7mX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a.complete([1,2,3,4,5,6,7,8,9,10], length=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHFy5tkNi6gB",
        "outputId": "fbfff6d9-cc5b-421f-8a3f-a4b1ee43ef7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "          1,  1],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10,  1,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "          1,  0],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10,  0,  1,  1,  1,  1,  1,  1,  1,\n",
              "          1,  1]])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In contrast, counting doesn't seem to be a pattern recognised. "
      ],
      "metadata": {
        "id": "r9zDb7HssdHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = '''\n",
        "nucleus sampling also called top p sampling is a more advanced\n",
        "version of top sampling that results in a more consistent sampling \n",
        "performance it cuts between selectable and non selectable tokens \n",
        "based on the sum of their probabilities totaled bar heights when \n",
        "going from left to right on the linked picture until the specified \n",
        "cut value p is reached as opposed to top k sampling which cuts \n",
        "based on position index it can be used similarity to top k sampling \n",
        "you can combine low value of nucleus sampling with high value of \n",
        "randomness while the other sampling methods set to off to break out\n",
        "of loops with coherence once you ve got your head around how these \n",
        "work you can try layering settings it is difficult to explain \n",
        "sampling methods in simple terms without getting sloppy but it is \n",
        "not a complicated concept try to visualize the process using the \n",
        "bars as crutches randomness controls bar heights while sampling \n",
        "controls where the dividing line will be between bars that can be \n",
        "selected and bars tokens that will be discarded from participating in the'''.replace('\\n', ' ').replace('  ', ' ')"
      ],
      "metadata": {
        "id": "aiCI7RH-vRMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_chunks = [prompt[i:i+2] for i in range(0, len(prompt), 2)]"
      ],
      "metadata": {
        "id": "yBV9JOk68Vtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = list(set(prompt_chunks))"
      ],
      "metadata": {
        "id": "DWBcTijA7WTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dk4ahOef8dIk",
        "outputId": "5cec35bd-d876-4da7-bc83-7e383f4192a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "184"
            ]
          },
          "metadata": {},
          "execution_count": 606
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_tokenized = [vocabulary.index(word) for word in prompt_chunks]"
      ],
      "metadata": {
        "id": "lNQQs2VavSSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(bytes(prompt_tokenized))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPJwE0tu8-fJ",
        "outputId": "647683b7-0cda-4842-b2d2-a7f40d8cf305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1054"
            ]
          },
          "metadata": {},
          "execution_count": 381
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(zlib.compress(bytes(prompt_tokenized)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1VVbDRuvp-n",
        "outputId": "c2f3ccb6-8eae-47e5-e6e2-f05228d39c6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "505"
            ]
          },
          "metadata": {},
          "execution_count": 382
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def get_next_token(prompt, n=5, width=2, height=2):\n",
        "\n",
        "    prompt = list(prompt)\n",
        "\n",
        "    for i in range(n):\n",
        "        completions = a.complete(prompt, width=width, height=height, length=len(prompt) + 3)\n",
        "        completion = random.choice(completions)[-3 :]\n",
        "        prompt += completion\n",
        "\n",
        "    #out = ''.join([vocabulary[val] for val in prompt])\n",
        "    #return out\n",
        "\n",
        "    return prompt\n",
        "    "
      ],
      "metadata": {
        "id": "3aKhkoTuzV5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def complete(x): return bytes(a.complete(list(bytes(x, 'ascii')), width=2, height=5, length=len(x) + 3)[0])"
      ],
      "metadata": {
        "id": "lk0BycNusMuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bytes(get_next_token(bytes(prompt, 'ascii'), width=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OmfjMH2FKL-",
        "outputId": "dbbf3a70-dd31-49dc-ae70-548ebd418afa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b' nucleus sampling also called top p sampling is a more advanced version of top sampling that results in a more consistent sampling performance it cuts between selectable and non selectable tokens based on the sum of their probabilities totaled bar heights when going from left to right on the linked picture until the specified cut value p is reached as opposed to top k sampling which cuts based on position index it can be used similarity to top k sampling you can combine low value of nucleus sampling with high value of randomness while the other sampling methods set to off to break out of loops with coherence once you ve got your head around how these work you can try layering settings it is difficult to explain sampling methods in simple terms without getting sloppy but it is not a complicated concept try to visualize the process using the bars as crutches randomness controls bar heights while sampling controls where the dividing line will be between bars that can be selected and bars tokens that will be discarded from participating in the jx e jx erxyht'"
            ]
          },
          "metadata": {},
          "execution_count": 496
        }
      ]
    }
  ]
}
